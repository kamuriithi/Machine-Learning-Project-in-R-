---
title: "Introduction to Malaria Modeling Skills in R & RStudio"
author: "D.K.MURIITHI"
date: "`r Sys.Date()`"
output:
  html_document:  
  word_document: default
  pdf_document: default
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.height = 4,
	fig.width = 8,
	message = FALSE,
	warning = FALSE,
	comment = NA)
```

## Confirmation and setting of working directory

```{r}
setwd("~/TMMS2024")
```

## Installation and loading of necessary packages/libraries

# Loading libraries
```{r}
library(caret) #for training machine learning models
library(psych) ##for description of  data
library(ggplot2) ##for data visualization
library(caretEnsemble)##enables the creation of ensemble models
library(tidyverse) ##for data manipulation
library(mlbench)  ## For benchmarking ML Models
library(flextable) ## To create and style tables
library(mltools) #for hyperparameter tuning
library(tictoc) #for determining the time taken for a model to run
library(ROSE)  ## for random oversampling
library(smotefamily) ## for smote sampling
library(ROCR) ##For ROC curve
library(pROC) ## For visualizing, smoothing, and comparing ROC curves
library(e1071) ## For statistical modeling and  machine learning tasks
library(class) ## For classification using k-Nearest Neighbors and other methods
library(caTools) ## For splitting data into training and testing sets
library(MASS) ## Provides plotting functions and datasets
library(ISLR) ## for practical applications of statistical learning methods
library(boot) ## Useful for performing bootstrap resampling
library(cvTools) ## Contains functions for cross-validation, bootstrapping, and other resampling methods
```


## Loading the given Malaria data

```{r}
mdata = read.csv("final_malaria_survey_data.csv", header = TRUE)
head(mdata,4)
```
## Exporatory of the dataset
```{r}
#dim(mdata)      ## View the Dimension of the Data
#names(mdata)     ## View the variable/features/column names
#summary(mdata)    ## Descriptive Statistics
#describe(mdata)   ## Descriptive Statistics
#sum(is.na(mdata))  ## Check for missing data
#na.omit(mdata)     ## Remove rows with any missing values
#is.na(Malaria.Test)  ## checks for missing values in the Malaria.Test column of your data frame
```

## Note: For the purpose of this training: It is assumed that the data is already clean and preprocessed 

# Plot Target variable using R Base function
```{r}
plot(factor(mdata$Malaria.Test),
     names= c("Negative", "Positive"), 
     col=c("green","red"), 
     ylim=c(0, 3000), ylab= "Respondent", xlab= "Malaria Diagnosis", main = "Malaria Diagnosis Plot")
#box()
```


# Plot Target variable using ggplot function
```{r}
ggplot(mdata, aes(x = Malaria.Test, fill = Malaria.Test)) + 
  geom_bar() + 
  labs(x = "Malaria Test", 
       y = "Respondent",
       tittle = "Malaria Diagnosis Results",
       caption = "Source: KNBS 2021 Data") +
    theme_classic()
```

# Check for zero variance predictors:
```{r}
nzv <- nearZeroVar(mdata[,-14], saveMetrics = TRUE) ## Function called nearZeroVar and captures its output in the variable nzv
nzv
```

# Remove nzv
```{r}
mdata1 <- mdata[, !nzv$nzv] ## Removing features with little to no variation
dim(mdata1)
```

# Set the seed for reproducibility
```{r}
set.seed(123) ## This line sets the random seed for the analysis
```

* Random seeds are used to ensure reproducibility. 

* By setting the seed to 2024, you're telling the program to always start with the same "random" starting point when generating random numbers needed for the analysis. 

* This is helpful for debugging or comparing results across different runs.
## DATA PARTITION FOR MACHINE LEARNING

```{r}
set.seed(123)
index = sample(2, nrow(mdata1),replace =T, prob=c(0.70,0.30))
train = mdata1[index ==1,]
test = mdata1[index ==2,]
```

# Get the dimensions of your train and test data
```{r}
dim(train)
dim(test)
```

## VIEW THE MODELS IN CARET

```{r}
models= getModelInfo()
#names(models)
```

# Prepare training scheme for cross-validation 

# Cross-validation
This involves splitting the data into multiple subsets (folds), training the model on some folds, and testing it on    the remaining fold. The process is repeated for each fold
# Repeated cross-validation 
This involves performing cross-validation multiple times to reduce the variability of the results reducing the likelihood of overfitting

```{r}
control <- trainControl(method="repeatedcv", number=10, repeats=5)
```


```{r}
#models= getModelInfo()
#names(models)
```

# TRAIN OR BUILD MACHINE LEARNING MODELS
The model is trained until it can detect the underlying patterns and relationships, enabling it to yield good results when presented with unseen data.

# Train a Support Vector Machine (SVM) model
# Support Vector Machine (SVM) 
SVM is a supervised machine learning algorithm used for classification and regression tasks. 
The primary goal of SVM is to find a hyperplane that best divides a dataset into classes.

# Types of Support Vector Machine 
# Linear SVM
Used when data is linearly separable. It finds a straight hyperplane that separates the data into classes.

# Non-linear SVM
Used when data is not linearly separable. It employs kernel functions to map data into higher dimensions to find a hyperplane.

# Applications of SVM
  = Image recognition
  = Text classification
  = Healthcare (e.g., malaria test results classification)
  = Handwriting recognition

# Train a Support Vector Machine (SVM) model
```{r}

```


```{r}
set.seed(123)
tic()
SvmModel <- train(factor(Malaria.Test)~., 
                  data=train, 
                  method="svmRadial", 
                  preProcess= c("scale", "center"), 
                  trControl=control,
                  tuneLength=10, 
                  na.action = na.omit)
toc()
SvmModel
```

# C:

This parameter is known as the regularization parameter in SVMs.
It controls the trade-off between maximizing the margin (the gap between the hyperplane and the closest data points) and minimizing the misclassification penalty.

A lower C value (like 0.25) implies a softer margin, allowing for some misclassifications but prioritizing a larger margin. This can be useful for datasets with noise or outliers.

# sigma (Î³):

This parameter controls the spread of the Gaussian function in the RBF kernel.
A lower sigma value (like 0.1164714) corresponds to a wider Gaussian function, which essentially considers data points farther away during the decision boundary formation.
This can be helpful for capturing smoother, less complex non-linear relationships between data points

# Prediction using Trained SVM Model
```{r}
Svmpred= predict(SvmModel,newdata = test)

# Combine data into a data frame
Ground_truth<- test$Malaria.Test
Predicted <- Svmpred

resultSvm <- data.frame(Ground_truth, Predicted)
resultSvm$Correct <- resultSvm$Ground_truth == resultSvm$Predicted

# Add a column for classification results (correct/incorrect)
resultSVM<- data.frame(test, Svmpred, resultSvm$Correct)
#print(resultSvm,150)
```

# Evaluation of SVM model performance metrics
```{r}
SVM_CM<- confusionMatrix(Svmpred,as.factor(test$Malaria.Test), positive = "Positive", mode="everything")
SVM_CM
M1 <- SVM_CM$byClass[c(1, 2, 5, 7, 11)]
M1
```

# Train a Random Forest model

# Random Forests 
This is an ensemble learning method that combines multiple decision trees to improve prediction accuracy and reduce variance.
# mtry
This parameter controls the number of features randomly chosen as candidates for splitting a node in each tree.

```{R}
set.seed(123)
tic()
RFModel <- train(factor(Malaria.Test)~., 
                 data=train, 
                 method="rf", 
                 trControl=control, 
                 na.action = na.omit)
toc()
RFModel
```



```{r}
# Prediction using RF model
RFpred=predict(RFModel,newdata = test)

# Evalustion of the RF model performance mentrics
RF_CM<- confusionMatrix(RFpred,as.factor(test$Malaria.Test), positive = "Positive", mode='everything')
RF_CM
M2<- RF_CM$byClass[c(1, 2, 5, 7, 11)]
M2
```
# Plotting Random Forest confusion matrix
```{r}
fourfoldplot(RF_CM$table, col=rainbow(4), main="RF Confusion Matrix") #RF Confusion Matrix 4fold plot
```

# Show relative importance of features
```{r}
# Plot using R base function
plot(varImp(RFModel, scale=T))

# Alternatively
vip::vip(RFModel)

# Alternatively using ggplot function
var_imp <-varImp(RFModel)
ggplot(var_imp, aes(x = reorder(Variable, Importance), y = importance)) +
  geom_bar(stat = "identity", fill = "tomato") +
  coord_flip() +
  xlab("Variable") +
  ylab("Importance") +
  ggtitle("Feature Importance Plot for RF Model")
```

# Create ROC curve for RF model
```{r}
# Make predictions on the test set using type='prob'
predrf <- predict(RFModel, newdata = test, type = "prob")
# Create a prediction object needed by ROCR
pred_rf <- prediction(predrf[, "Positive"], test$Malaria.Test)
# Calculate performance measures like ROC curve
perf_rf <- performance(pred_rf, "tpr", "fpr")
# Plot the ROC curve
plot(perf_rf, colorize = TRUE, main = "ROC Curve-Random Forest")
# Compute AUC
auc_value <- performance(pred_rf, "auc")@y.values[[1]]
auc_label <- paste("AUC =", round(auc_value, 2))
# Add AUC value as text on the plot
text(0.5, 0.3, auc_label, col = "red", cex = 1.5)  # Adjust position
```

# Receiver Operating Characteristic (ROC) Curve

The ROC curve is a graphical representation of the performance of a classification model at all classification thresholds. The curve plots the True Positive Rate (TPR) against the False Positive Rate (FPR).

# True Positive Rate (TPR): Also known as sensitivity or recall, 
It is the ratio of correctly predicted positive observations to the actual positives. 
TPR = TP / (TP + FN), where TP is True Positives and FN is False Negatives.

# False Positive Rate (FPR)
It is the ratio of incorrectly predicted positive observations to the actual negatives. 
FPR = FP /(FP + TN), where FP is False Positives and TN is True Negatives.

# The AUC is the area under the ROC curve. 
It ranges from 0 to 1, with higher values indicating better model performance. 
AUC = 0.5 suggests no discrimination (i.e., the model is no better than random guessing), 
AUC = 1.0 indicates perfect discrimination.

AUC = 0.99 is very close to 1.0, suggesting that the model has a high true positive rate and a low false positive rate. This indicates that the Random Forest classifier has an excellent ability to distinguish between the positive and negative classes.
  
# Train the Decision Tree

```{r}
set.seed(123)
tic()
DTModel <- train(factor(Malaria.Test)~., data=train, method="rpart", trControl=control)
toc()
DTModel
DTpred=predict(DTModel,newdata = test)
DT.cM<- confusionMatrix(DTpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
DT.cM
### Prediction
T2<- DT.cM$byClass[c(1, 2, 5, 7, 11)]
T2
```

# plotting confusion matrix
```{R}
DT.cM$table
fourfoldplot(DT.cM$table, col=rainbow(4), main="Imbalanced DT Confusion Matrix")
plot(varImp(DTModel, scale=T))
vip::vip(DTModel)
```


# Train an Logisitic Regression model
```{r}
set.seed(123)
logRegModel <- train(factor(Malaria.Test)~., 
                     data=train, 
                     method="glm", 
                     trControl=control, 
                     na.action = na.omit)
#logRegModel
```

# Prediction using trained Logisitic Regression model
```{r}
logRegpred=predict(logRegModel,newdata = test)

logRegPredProb <- predict(logRegModel, newdata = test, type ="prob")*100

# Combine data into a data frame
Ground_truth<- test$Malaria.Test
Predicted <- logRegpred

resultLR <- data.frame(Ground_truth, Predicted)
resultLR$Correct <- resultLR$Ground_truth == resultLR$Predicted

# Add a column for classification results (correct/incorrect)
resultLogreg<- data.frame(test, logRegpred, resultLR$Correct)
#head(resultLogreg)
#head(resultLR, 200)
```

# Evaluation of Logisitic Regression model performance metrics
```{r}
logReg_CM<- confusionMatrix(logRegpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
logReg_CM
M3<- logReg_CM$byClass[c(1, 2, 5, 7, 11)]
M3

#plotting confusion matrix
logReg_CM$table
fourfoldplot(logReg_CM$table, col=rainbow(4), main="LR Confusion Matrix")
```


```{r}
plot(varImp(logRegModel, scale=T))
vip::vip(logRegModel)
# Alternatively using ggplot function
varImp <-varImp(logRegModel)
ggplot(varImp, aes(x = reorder(Variable, Importance), y = importance)) +
  geom_bar(stat = "identity", fill = "tomato") +
  coord_flip() +
  xlab("Variable") +
  ylab("Importance") +
  ggtitle("Feature Importance Plot for LogisticRegression Model")
```

```{r}
# Make predictions on the test set using type='prob'
predlogReg <- predict(logRegModel, newdata = test, type = "prob")
# Create a prediction object needed by ROCR
pred_logReg <- prediction(predlogReg[, "Positive"], test$Malaria.Test)
# Calculate performance measures like ROC curve
perf_logReg <- performance(pred_logReg, "tpr", "fpr")
# Plot the ROC curve
plot(perf_logReg, colorize = TRUE, main = "ROC Curve-Logistic Regression")
# Compute AUC
auc_value <- performance(pred_logReg, "auc")@y.values[[1]]
auc_label <- paste("AUC =", round(auc_value, 2))
# Add AUC value as text on the plot
text(0.5, 0.3, auc_label, col = "blue", cex = 1.5)  # Adjust position and other text parameters as needed
```


# Train a k- Nearest Neigbour Model

#k-Nearest Neighbors (k-NN) 
k-NN is a simple, non-parametric, and lazy learning algorithm used for both classification and regression tasks.

# k
In kNN, k refers to the number of nearest neighbors considered for classifying a new data point. If k = 3, the model would classify new data points based on the majority vote of their 7 closest neighbors in the training data.

```{r}
set.seed(123)
knnModel <- train(factor(Malaria.Test)~., 
                  data = train, 
                  method ="knn", 
                  tuneGrid = data.frame(k = seq(1, 20, 2)),
                  trControl = control)
knnModel
#ggplot(knnModel, highlight = TRUE)
```

# Prediction using knnModel
```{r}
knnpred = predict(knnModel,newdata = test)
```

# Evaluation of model performance metrics
```{r}
knn_CM<- confusionMatrix(knnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M4<- knn_CM$byClass [c(1, 2, 5, 7, 11)]
M4

#plotting confusion matrix
knn_CM$table
fourfoldplot(knn_CM$table, col=rainbow(4), main="KNN Confusion Matrix")
```

# Train a Neural Net model

# Neural Network (NN) 
**NN is a computational model inspired by the way biological neural networks in the human brain process information. 
**They consist of interconnected layers of nodes, or neurons, which work together to perform complex tasks such as       classification, regression, and more. 
**Neural Networks are the foundation of deep learning and have proven highly effective in tasks involving image          recognition

```{r}
set.seed(123)
tic()
nnModel <- train(factor(Malaria.Test)~., data=train, method="nnet", trControl=control)
toc()
#nnModel
nnpred=predict(nnModel,newdata = test)
nn_CM<- confusionMatrix(nnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M5<- nn_CM$byClass[c(1, 2, 5, 7, 11)]
M5
#plotting confusion matrix
nn_CM$table
fourfoldplot(nn_CM$table, col=rainbow(4), main="Neural Network Confusion Matrix")
plot(varImp(nnModel, scale=T))
```

```{r}
library(NeuralNetTools)
plotnet(nnModel$finalModel)
```

The image above depicts a neural network visualization generated using the NeuralNetTools package in R. This network is a simple feed forward neural network with an input layer, a hidden layer, and an output layer. Below is an explanation of the various components of this neural network:

## Components of the Neural Network:
### Input Layer (I1 to I17):

Each node in the input layer represents a feature from the mdata. In this case, there are 17 input features, labeled I1 to I17.
These features could be various symptoms and indicators related to severe malaria, such as age, sex, fever, cold, rigor, etc.

### Hidden Layer (H1):
The hidden layer has one node, labeled H1. The lines connecting the input nodes (I1 to I17) to the hidden node H1 represent the weights of the connections between these layers. The thickness and color of these lines indicate the strength and polarity (positive or negative) of the weights.

### Output Layer (O1):
The output layer has one node, labeled O1, which represents the predicted output of the model. In this binary classification problem, the output might be the probability of having severe malaria.

### Bias Nodes (B1, B2):
Bias nodes B1 and B2 provide an additional parameter to the model, helping it better fit the data. B1 is connected to the hidden layer, and B2 is connected to the output layer.

### Interpretation of Weights:
### Connection Weights:
The weights of connections between layers are shown as lines with varying thickness and color. Thick lines indicate strong weights, while thin lines indicate weaker weights. The color (e.g., black or gray) might indicate the sign of the weight (positive or negative).

## Explanation of the Network Functioning:
### Input to Hidden Layer:
Each input feature is multiplied by its corresponding weight and passed to the hidden node H1. The hidden node H1 sums these weighted inputs along with the bias B1.

### Hidden Layer Activation:
The hidden node H1 applies an activation function (commonly a nonlinear function like sigmoid or ReLU) to the summed inputs to introduce nonlinearity into the model.

### Hidden to Output Layer:
The activated output of H1 is multiplied by the weight of the connection to the output node O1 and passed to O1. Similarly, the bias B2 is also added to the output node O1.

### Output Layer Activation:
The output node O1 might apply another activation function to produce the final prediction, such as a probability score in the case of binary classification.

The neural network uses the input features to predict the output through weighted connections and biases. The hidden layer allows the model to capture complex relationships between the input features. The visualization helps in understanding the network structure and the importance of different features in the prediction process.

# Train a Naive Bayes model

```{r}
set.seed(123)
NBModel <- train(factor(Malaria.Test)~., data=train, method="nb",trControl=control)
NBModel
NBpred=predict(NBModel,newdata = test)
NB_CM<- confusionMatrix(NBpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M6<- NB_CM$byClass[c(1, 2, 5, 7, 11)]
M6
#plotting confusion matrix
NB_CM$table
fourfoldplot(NB_CM$table, col=rainbow(4), main="Naive Bayes Confusion Matrix")
```

## Train a Linear Discriminant Analysis model

```{r}
set.seed(123)
LDAModel <- train(factor(Malaria.Test)~., data=train, method="lda", trControl=control)
LDAModel
LDApred=predict(LDAModel,newdata = test)
LDA_CM<- confusionMatrix(LDApred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M7<- LDA_CM$byClass[c(1, 2, 5, 7, 11)]
M7
#plotting confusion matrix
LDA_CM$table
fourfoldplot(LDA_CM$table, col=rainbow(4), main="LDA Confusion Matrix")
```

# Train a Linear Vector Quantization model
```{r}
set.seed(123)
LVQModel <- train(factor(Malaria.Test)~., data=train, method="lvq", trControl=control)
LVQModel
LVQpred=predict(LVQModel,newdata = test)
LVQ_CM<- confusionMatrix(LVQpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
LVQ_CM
M8<- LVQ_CM$byClass[c(1, 2, 5, 7, 11)]
M8
#plotting confusion matrix
LVQ_CM$table
fourfoldplot(LVQ_CM$table, col=rainbow(4), main="LDA Confusion Matrix")
```

# Train a Bagging model

```{r}
set.seed(123)
tic()
bagModel <- train(factor(Malaria.Test)~., data=train, method="treebag", trControl=control)
toc()
bagModel
bagpred=predict(bagModel,newdata = test)
bag_CM<- confusionMatrix(bagpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M9<- bag_CM$byClass[c(1, 2, 5, 7, 11)]
M9
#plotting confusion matrix
bag_CM$table
fourfoldplot(bag_CM$table, col=rainbow(4), main="Bagging Confusion Matrix")
plot(varImp(bagModel, scale=T))
```
# Train a Boosting model
```{r}
set.seed(123)
tic()
boModel <- train(factor(Malaria.Test)~., data=train, method="ada", trControl=control)
toc()
boModel
bopred=predict(boModel,newdata = test)
bo_CM<- confusionMatrix(bopred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
M10<- bo_CM$byClass[c(1, 2, 5, 7, 11)]
M10
#plotting confusion matrix
bo_CM$table
fourfoldplot(bo_CM$table, col=rainbow(4), main="Boosting Confusion Matrix")
```

# Prediction using trained Boosting model
```{r}
bopred=predict(boModel,newdata = test)

boPredProb <- predict(boModel, newdata = test, type ="prob")*100

# Combine data into a data frame
Ground_truth<- test$Malaria.Test
Predicted <- bopred

resultbo <- data.frame(Ground_truth, Predicted)
resultbo$Correct <- resultbo$Ground_truth == resultbo$Predicted

# Add a column for classification results (correct/incorrect)
resultBoosting<- data.frame(test, bopred, resultbo$Correct)
#head(resultBoosting)
#head(resultbo,987)
```


# Tabulate your Results [xtable(measure.score, digits = 3)]
```{r}
measure <-round(data.frame(SVM = M1, 
                           RF = M2, 
                           DT = T2, 
                           LR = M3, 
                           KNN = M4, 
                           NN = M5, 
                           NB = M6, 
                           LDA = M7, 
                           LVQ = M8, 
                           Bagging = M9, 
                           Boosting = 10), 3) 
rownames(measure)=c('Sensitivity', 'Specificity', 'Precision','F1-Score', 'Balanced Accuracy')
measure
```

# Collect all resamples and compare the models
```{r}
results <- resamples(list(SVM=SvmModel, 
                          RF=RFModel,
                          DT= DTModel,
                          LR=logRegModel,
                          knn=knnModel,
                          NB=NBModel,
                          LDA=LDAModel,
                          LVQ=LVQModel,
                          Bagging=bagModel,
                         Bo=boModel ))
# summarize the distributions of the results 
summary(results)
```

```{r}
# boxplots of results
bwplot(results)
```
# Box-and-whisker plot of results
This type of chart is used to visualize the distribution of data. It shows the following information:
 * The center of the data (usually the median)
 * The spread of the data (represented by the box)
 * The presence of any outliers (data points that fall outside a certain range)

```{r}
# dot plots of results
 dotplot(results)
```

```{r}

```

