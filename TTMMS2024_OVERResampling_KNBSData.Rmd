---
title: "Resampling Techniques for Handling Malaria Imbalance Data"
author: "D.K.MURIITHI"
date: "2024-07-28"
output:
  html_document:  
  word_document: default
  pdf_document: default
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 6,	message = FALSE, warning = FALSE,	comment = NA)
```
## confirmation and setting of working directory

```{r}
setwd("C:\\Users\\Prof DK\\Desktop\\TMMS2024")
```


## Installation and loading of necessary packages/libraries

# Loading libraries
```{r}
library(caret) #for training machine learning models
library(psych) ##for description of  data
library(ggplot2) ##for data visualization
library(caretEnsemble)##enables the creation of ensemble models
library(tidyverse) ##for data manipulation
library(mlbench)  ## For benchmarking ML Models
library(flextable) ## To create and style tables
library(mltools) #for hyperparameter tuning
library(tictoc) #for determining the time taken for a model to run
library(ROSE)  ## for random oversampling
library(smotefamily) ## for smote sampling
library(ROCR) ##For ROC curve
library(pROC) ## For visualizing, smoothing, and comparing ROC curves
library(e1071) ## For statistical modeling and  machine learning tasks
library(class) ## For classification using k-Nearest Neighbors and other methods
library(caTools) ## For splitting data into training and testing sets
library(MASS) ## Provides plotting functions and datasets
library(ISLR) ## for practical applications of statistical learning methods
library(boot) ## Useful for performing bootstrap resampling
library(cvTools) ## Contains functions for cross-validation, bootstrapping, and other resampling methods
```

# ----------------------------------------------------
## Resampling Techniques for Handling Data Imbalance
# ----------------------------------------------------
● Oversampling
● Undersampling
● Combined Resampling

 * Resampling techniques are a common set of strategies used to address data imbalance in machine         learning. 
 
 * These techniques involve modifying the dataset by either increasing the number of minority class       samples (oversampling) or 
 * reducing the number of majority class samples (undersampling). Here are some key resampling            techniques:
 
# 1. Oversampling:

#● Random Oversampling: 
In this method, random instances from the minority class are duplicated until a more balanced distribution is achieved. While this can balance the class distribution, it may lead to overfitting.
  
#● SMOTE (Synthetic Minority Over-sampling Technique): 
SMOTE generates synthetic instances for the minority class by interpolating between neighboring instances. This approach creates new, realistic data points and helps prevent overfitting compared to random oversampling.

#● ADASYN (Adaptive Synthetic Sampling)
  * Description: An extension of SMOTE that focuses on generating more synthetic data for minority class examples that are harder to learn.
  * Advantages: Improves the focus on difficult minority class examples, potentially enhancing model performance.
  * Disadvantages: Similar to SMOTE, it can introduce noise if not applied carefully.

#● SMOTEN
#● SVM-SMOTE 
#● Random oversampler
#● Kmeans-SMOTE

# 2. Undersampling

#● Random Undersampling
  * Description: Involves randomly removing examples from the majority class to balance the dataset.
  * Advantages: Reduces the size of the dataset, making the training process faster.
  * Disadvantages: Can lead to loss of valuable information and underfitting.

#● Tomek Links
  *Description: Removes examples from the majority class that are close to minority class examples, forming Tomek links.
  *Advantages: Helps clean the boundary between classes, improving model performance.
  *Disadvantages: Only removes a small number of majority class examples, may not fully balance the dataset.
  
#● Random undersampler  
#● NearMiss
#● condensed Nearest Neighbour
#● Edited Nearest Neaghbour
  
# 3. Combined Resampling

#● Hybrid Methods
 *Description: Combines several resampling techniques to leverage their strengths and mitigate their weaknesses.
 *Advantages: Can provide a more balanced and effective approach.
 *Disadvantages: More complex to implement and require careful tuning.
 
## Consideration for Effective Resampling
* Understand Your Data: Know the extent and impact of imbalance.
* Evaluate Multiple Techniques: Different techniques might work better for different datasets.
* Cross-Validation: Use cross-validation to ensure that the model generalizes well.
* Performance Metrics: Focus on metrics like F1-score, precision, recall, and AUC-ROC instead of accuracy.

# ---------------------------------------------------- 
## Loading the given Malaria data
# ----------------------------------------------------
```{r}
mdata = read.csv("final_malaria_survey_data.csv", header = TRUE)
head(mdata,4)
```

```{r}
attach(mdata)
dim(mdata)
head(mdata,5)
names(mdata)
str(mdata)
summary(mdata)   ###Descriptive Statistics
describe(mdata)  ###Descriptive Statistics
sum(is.na(mdata)) ###Check for missing data
```
# Plot Target Variable
```{r}
plot(factor(Malaria.Test), 
     names= c("Negative", "Positive"), 
     col=c(2,3), 
     ylim=c(0, 3000), ylab= "Respondent", xlab= "Malaria Diagnosis", main = "Malaria Diagnosis Results")
#box()
```

# Alternatively use of ggplot 
```{r}
ggplot(mdata, aes(x = factor(Malaria.Test), fill = Malaria.Test)) + 
  geom_bar() + 
  labs(x = "Malaria Diagnosis", y = "Respondent") +
  theme_classic()
```
# Set the seed for reproducibility
```{r}
set.seed(2024) ## This line sets the random seed for the analysis
```

* Random seeds are used to ensure reproducibility. 

* By setting the seed to 2024, you're telling the program to always start with the same "random" starting point when generating random numbers needed for the analysis. 

* This is helpful for debugging or comparing results across different runs.

# -------------------------------------------------
## DATA PARTITION
# -------------------------------------------------
```{r}
set.seed(2024)
index=sample(2, nrow(mdata),replace =TRUE, prob=c(0.70,0.30))
train=mdata[index==1,]
test= mdata[index==2,]
#Get the dimensions of your train and test data
dim(train)
dim(test)
```
# Now Let's train some machine learning models using package caret

* The caret R package (short for Classification and regression Training) to carry out machine learning tasks in RStudio

* The caret package offers a range of tools and models for classification and regression machine learning problems(Kuhn et al. 2021)

* In fact, it offers over 239 different machine learning models from which to choose. 

* Don’t worry, we don’t expect you to use them all!

# VIEW THE MODELS IN CARET

```{r}
models= getModelInfo()
#names(models)
```

# ----------------------------------------------------------------------------------------------
### Handle Imbalanced: Oversampled data 
# ----------------------------------------------------------------------------------------------

```{r}
over <- ovun.sample(factor(Malaria.Test)~., data = train, method = "over")$data
dim(over)
```


# Plot Target variable using ggplot function
```{r}
ggplot(over, aes(x = Malaria.Test, fill = Malaria.Test)) + 
  geom_bar() + 
  labs(x = "Malaria Test", 
       y = "Respondent",
       tittle = "Malaria Diagnosis Results",
       caption = "Source: KNBS 2021 Data") +
    theme_classic()
```
# ----------------------------------------------------------------------------------------------
## Building Machine Learning Models
# ----------------------------------------------------------------------------------------------

# prepare training scheme for cross-validation
```{r}
 control <- trainControl(method="repeatedcv", number=10, repeats=5)
```

# Train an SVM model 
```{r}
 set.seed(2024)
 tic()
 over.svmModel <- train(Malaria.Test~., data=over, method="svmRadial", trControl=control)
 toc()
 over.svmModel
 over.svmpred=predict(over.svmModel,newdata = test)
 over.SVM.cM<- confusionMatrix(over.svmpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
 over.SVM.cM
 over.m1<- over.SVM.cM$byClass[c(1, 2, 5, 7, 11)]
 over.m1
 #plotting confusion matrix
 over.SVM.cM$table
 fourfoldplot(over.SVM.cM$table, col=rainbow(4), main="Oversampled SVM Confusion Matrix")
```

# Train an Random Forest model
```{r}
set.seed(2024)
tic()
over.RFModel <- train(Malaria.Test~., data=over, method="rf", trControl=control)
toc()
over.RFModel
over.RFpred=predict(over.RFModel,newdata = test)
over.RF.cM<- confusionMatrix(over.RFpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m2<- over.RF.cM$byClass[c(1, 2, 5, 7, 11)]
over.m2
#plotting confusion matrix
over.RF.cM$table
fourfoldplot(over.RF.cM$table, col=rainbow(4), main="Oversampled RF Confusion Matrix")
```

# Train a Logisitic Regression model
```{r}
set.seed(2024)
tic()
over.lrModel <- train(Malaria.Test~., data=over, method="glm", trControl=control)
toc()
over.lrModel
over.lrpred=predict(over.lrModel,newdata = test)
over.lr.cM<- confusionMatrix(over.lrpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m3<- over.lr.cM$byClass[c(1, 2, 5, 7, 11)]
over.m3
#plotting confusion matrix
over.lr.cM$table
fourfoldplot(over.lr.cM$table, col=rainbow(4), main="Oversampled LR Confusion Matrix")
```

# Train an k- Nearest Neigbour model
```{r}
set.seed(2024)
tic()
over.knnModel <- train(Malaria.Test~., data=over, method="knn", trControl=control)
toc()
over.knnModel
over.knnpred=predict(over.knnModel,newdata = test)
over.knn.cM<- confusionMatrix(over.knnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m4<- over.knn.cM$byClass[c(1, 2, 5, 7, 11)]
over.m4
#plotting confusion matrix
over.knn.cM$table
fourfoldplot(over.knn.cM$table, col=rainbow(4), main="Oversampled KNN Confusion Matrix")
```

# Train a Neural Net model
```{r}
set.seed(2024)
tic()
over.nnModel <- train(Malaria.Test~., data=over, method="nnet", trControl=control)
toc()
over.nnModel
over.nnpred=predict(over.nnModel,newdata = test)
over.nn.cM<- confusionMatrix(over.nnpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m5<- over.nn.cM$byClass[c(1, 2, 5, 7, 11)]
over.m5
#plotting confusion matrix
over.nn.cM$table
fourfoldplot(over.nn.cM$table, col=rainbow(4), main="Oversampled NN Confusion Matrix")
```

# Train a Naive Bayes model
```{r}
set.seed(2024)
tic()
over.nbModel <- train(Malaria.Test~., data=over, method="nb", trControl=control)
toc()
over.nbModel
over.nbpred=predict(over.nbModel,newdata = test)
over.nb.cM<- confusionMatrix(over.nbpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m6<- over.nb.cM$byClass[c(1, 2, 5, 7, 11)]
over.m6
#plotting confusion matrix
over.nb.cM$table
fourfoldplot(over.nb.cM$table, col=rainbow(4), main="Oversampled NB Confusion Matrix")
```

# Train a Linear Discriminant Analysis model
```{r}
set.seed(2024)
over.ldaModel <- train(Malaria.Test~., data=over, method="lda", trControl=control)
over.ldaModel
over.ldapred=predict(over.ldaModel,newdata = test)
over.lda.cM<- confusionMatrix(over.ldapred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m7<- over.lda.cM$byClass[c(1, 2, 5, 7, 11)]
over.m7
##plotting confusion matrix
over.lda.cM$table
fourfoldplot(over.lda.cM$table, col=rainbow(4), main="Imbalanced LDA Confusion Matrix")
```

# Train a Decision Tree model
```{r}
set.seed(2024)
over.DTModel <- train(Malaria.Test~., data=over, method="rpart", trControl=control)
over.DTModel
over.DTpred=predict(over.DTModel,newdata = test)
over.DT.cM<- confusionMatrix(over.DTpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m8<- over.DT.cM$byClass[c(1, 2, 5, 7, 11)]
over.m8
##plotting confusion matrix
over.DT.cM$table
fourfoldplot(over.DT.cM$table, col=rainbow(4), main="Imbalanced Decision Tree Confusion Matrix")
```

# Train a Bagging model
```{r}
set.seed(2024)
over.bagModel <- train(Malaria.Test~., data=over, method="treebag", trControl=control)
over.bagModel
over.bagpred=predict(over.bagModel,newdata = test)
over.bag.cM<- confusionMatrix(over.bagpred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m9<- over.bag.cM$byClass[c(1, 2, 5, 7, 11)]
over.m9
#plotting confusion matrix
over.bag.cM$table
fourfoldplot(over.bag.cM$table, col=rainbow(4), main="Oversampled Bagging Confusion Matrix")
```

# Train a Boosting model
```{r}
set.seed(2024)
tic()
over.boModel <- train(Malaria.Test~., data=over, method="ada", trControl=control)
toc()
over.boModel
over.bopred=predict(over.boModel,newdata = test)
over.bo.cM<- confusionMatrix(over.bopred,as.factor(test$Malaria.Test), positive = 'Positive', mode='everything')
over.m10<- over.bo.cM$byClass[c(1, 2, 5, 7, 11)]
over.m10
#plotting confusion matrix
over.bo.cM$table
fourfoldplot(over.bo.cM$table, col=rainbow(4), main="Oversampled Boosting Confusion Matrix")
```

############################### measure #########################################

```{r}
measure <-round(data.frame(SVM= over.m1, 
                                 RF= over.m2, 
                                 LR = over.m3, 
                                 KNN=over.m4, 
                                 NN=over.m5, 
                                 NB=over.m6, 
                                 LDA=over.m7, 
                                 DT=over.m8, 
                                 Bagging = over.m9, 
                                 Boosting= over.m10), 4)
rownames(measure)=c('Sensitivity', 'Specificity', 'Precision','F1-Score', 'Balanced Accuracy')
measure
```

# collect all resamples and compare
```{r}
results <- resamples(list(SVM=over.svmModel, 
                          RF=over.RFModel,
                          LR=over.lrModel,
                          KNN=over.knnModel,
                          nn=over.nnModel,
                          NB=over.nbModel,
                          LDA=over.ldaModel,
                          DT=over.DTModel,
                          Bagging=over.bagModel,
                          Boosting=over.boModel))
```

```{r}
library(dplyr)
## summarize the distributions of the results 
summary(results)
```

# Box-and-whisker plot of results
This type of chart is used to visualize the distribution of data. It shows the following information:
 * The center of the data (usually the median)
 * The spread of the data (represented by the box)
 * The presence of any outliers (data points that fall outside a certain range)
  
```{r}
bwplot(results)
```


```{r}
## dot plots of results
dotplot(results)
```

```{r}

```

